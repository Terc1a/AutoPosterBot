import requests
import json
import os
import logging
from urllib.parse import urlsplit, unquote
from bs4 import BeautifulSoup
from typing import List, Dict, Optional

USER_AGENT = "python:reddit.parser:v2.0 (by /u/Yar0v)"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ –¥–ª—è reddit_service
logger = logging.getLogger('reddit_service')
logger.setLevel(logging.DEBUG)


def download_media(url: str, folder: str = "downloads", index: int = 0) -> str:
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–π –º–µ–¥–∏–∞-—Ñ–∞–π–ª (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, gif, mp4) –ø–æ –ª—é–±–æ–π —Å—Å—ã–ª–∫–µ.
    –î–æ–±–∞–≤–ª–µ–Ω index –¥–ª—è —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ –≥–∞–ª–µ—Ä–µ–∏.
    """
    logger.info(f"üì• –ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–µ–¥–∏–∞ #{index} —Å URL: {url}")
    os.makedirs(folder, exist_ok=True)
    headers = {"User-Agent": USER_AGENT}

    try:
        # 1) –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å ‚Äî —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, —ç—Ç–æ —Å—Ä–∞–∑—É —Ñ–∞–π–ª –∏–ª–∏ HTML-—Å—Ç—Ä–∞–Ω–∏—Ü–∞
        logger.debug(f"üîç –í—ã–ø–æ–ª–Ω—è–µ–º GET –∑–∞–ø—Ä–æ—Å –∫ {url}")
        resp = requests.get(url, headers=headers, allow_redirects=True, timeout=15)
        resp.raise_for_status()
        ctype = resp.headers.get("Content-Type", "").lower()
        logger.debug(f"üìã Content-Type: {ctype}")

        # –ï—Å–ª–∏ —ç—Ç–æ HTML ‚Äî –∏—â–µ–º –ø—Ä—è–º–æ–π URL –Ω–∞ –º–µ–¥–∏–∞ –≤–Ω—É—Ç—Ä–∏
        if "text/html" in ctype:
            logger.debug("üåê –û–±–Ω–∞—Ä—É–∂–µ–Ω HTML, –ø–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É...")
            soup = BeautifulSoup(resp.text, "html.parser")

            # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤–∏–¥–µ–æ
            video = soup.find("video")
            if video:
                logger.debug("üé• –ù–∞–π–¥–µ–Ω —Ç–µ–≥ <video>")
                src = None
                if video.has_attr("src"):
                    src = video["src"]
                else:
                    src_tag = video.find("source")
                    if src_tag and src_tag.has_attr("src"):
                        src = src_tag["src"]
                if src:
                    logger.info(f"üîó –ù–∞–π–¥–µ–Ω –ø—Ä—è–º–æ–π URL –≤–∏–¥–µ–æ: {src}")
                    url = src
                    # –û–±–Ω–æ–≤–ª—è–µ–º content-type
                    sub_resp = requests.head(url, headers=headers, timeout=10)
                    ctype = sub_resp.headers.get("Content-Type", "video/mp4").lower()
            else:
                # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                img = soup.find("img")
                if img and img.has_attr("src"):
                    logger.debug("üñºÔ∏è –ù–∞–π–¥–µ–Ω —Ç–µ–≥ <img>")
                    src = img["src"]
                    logger.info(f"üîó –ù–∞–π–¥–µ–Ω –ø—Ä—è–º–æ–π URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {src}")
                    url = src
                    # –û–±–Ω–æ–≤–ª—è–µ–º content-type
                    sub_resp = requests.head(url, headers=headers, timeout=10)
                    ctype = sub_resp.headers.get("Content-Type", "image/jpeg").lower()

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª
        logger.debug(f"‚¨áÔ∏è –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª –ø–æ URL: {url}")
        file_resp = requests.get(url, headers=headers, stream=True, timeout=30)
        file_resp.raise_for_status()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        if "image/jpeg" in ctype or "image/jpg" in ctype:
            ext = "jpeg"
        elif "image/png" in ctype:
            ext = "png"
        elif "image/gif" in ctype:
            ext = "gif"
        elif "video/mp4" in ctype:
            ext = "mp4"
        else:
            # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ URL
            parsed_url = urlsplit(url)
            filename = os.path.basename(unquote(parsed_url.path))
            if "." in filename:
                ext = filename.split(".")[-1].lower()
            else:
                ext = "jpg"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
        parsed_url = urlsplit(url)
        base_name = os.path.basename(unquote(parsed_url.path))
        if not base_name or "." not in base_name:
            base_name = f"media_{index}.{ext}"
        else:
            # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∏ –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ
            base_name = base_name.split(".")[0] + f".{ext}"

        filepath = os.path.join(folder, base_name)
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫: {filepath}")

        # –°–∫–∞—á–∏–≤–∞–µ–º —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º
        total_size = int(file_resp.headers.get('content-length', 0))
        downloaded = 0

        with open(filepath, "wb") as f:
            for chunk in file_resp.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    if total_size > 0:
                        progress = (downloaded / total_size) * 100
                        if progress % 25 < 1:  # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 25%
                            logger.debug(f"‚è¨ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {progress:.1f}%")

        size_mb = os.path.getsize(filepath) / 1024 / 1024
        logger.info(f"‚úÖ –§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω: {filepath} ({size_mb:.2f} MB)")
        return filepath

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –º–µ–¥–∏–∞: {e}")
        raise


def process_reddit_post_data(post: Dict) -> Optional[Dict]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–¥–Ω–æ–≥–æ –ø–æ—Å—Ç–∞ Reddit –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –º–µ–¥–∏–∞-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π.
    
    Args:
        post: –¥–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç–∞ –∏–∑ Reddit API
        
    Returns:
        Optional[Dict]: –¥–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç–∞ —Å –º–µ–¥–∏–∞ –∏–ª–∏ None –µ—Å–ª–∏ –ø–æ—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –º–µ–¥–∏–∞
    """
    post_id = post["name"]
    title = post.get("title", "")
    is_self = post.get("is_self", False)

    logger.debug(f"üì∞ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ—Å—Ç: {post_id} - {title[:50]}...")

    media_paths = []
    media_type = None
    is_gallery = False

    if not is_self:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —ç—Ç–æ –≥–∞–ª–µ—Ä–µ—è?
        if post.get("is_gallery", False):
            logger.debug("üñºÔ∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –≥–∞–ª–µ—Ä–µ—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            is_gallery = True
            media_type = "gallery"

            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≥–∞–ª–µ—Ä–µ–∏
            gallery_data = post.get("gallery_data", {})
            media_metadata = post.get("media_metadata", {})

            if gallery_data and media_metadata:
                items = gallery_data.get("items", [])
                logger.debug(f"üì∏ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –≥–∞–ª–µ—Ä–µ–µ: {len(items)}")

                for idx, item in enumerate(items):
                    media_id = item.get("media_id")
                    if media_id and media_id in media_metadata:
                        media_info = media_metadata[media_id]

                        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                        if "s" in media_info:
                            # –ü–æ–ª—É—á–∞–µ–º URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                            if "u" in media_info["s"]:
                                # URL –≤ —Ñ–æ—Ä–º–∞—Ç–µ preview, –Ω—É–∂–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å
                                preview_url = media_info["s"]["u"]
                                # –ó–∞–º–µ–Ω—è–µ–º preview.redd.it –Ω–∞ i.redd.it –∏ —É–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                                image_url = preview_url.replace("preview.redd.it", "i.redd.it")
                                image_url = image_url.split("?")[0].replace("&amp;", "&")
                            elif "gif" in media_info["s"]:
                                image_url = media_info["s"]["gif"]
                            else:
                                continue

                            logger.debug(f"üîó URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è #{idx}: {image_url}")

                            try:
                                media_path = download_media(image_url, index=idx)
                                media_paths.append(media_path)
                            except Exception as e:
                                logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ #{idx}: {e}")
            else:
                logger.debug("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –¥–∞–Ω–Ω—ã–µ –≥–∞–ª–µ—Ä–µ–∏")
        else:
            # –û–±—ã—á–Ω—ã–π –ø–æ—Å—Ç —Å –æ–¥–Ω–∏–º –º–µ–¥–∏–∞
            url = post.get("url")
            logger.debug(f"üìé –û–±—ã—á–Ω—ã–π –º–µ–¥–∏–∞-–ø–æ—Å—Ç: {url}")

            try:
                media_path = download_media(url)
                media_paths = [media_path]

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–µ–¥–∏–∞
                ext = media_path.lower().split('.')[-1]
                if ext in ("jpg", "jpeg", "png"):
                    media_type = "image"
                elif ext == "mp4":
                    media_type = "video"
                elif ext == "gif":
                    media_type = "gif"

                logger.debug(f"üìã –¢–∏–ø –º–µ–¥–∏–∞: {media_type}")
            except Exception as e:
                logger.debug(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –º–µ–¥–∏–∞: {e}")

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –º–µ–¥–∏–∞-—Ñ–∞–π–ª—ã
    if media_paths:
        result = {
            "post_id": post_id,
            "title": title,
            "media_type": media_type,
            "media_paths": media_paths,
            "is_gallery": is_gallery
        }
        logger.debug(f"‚úÖ –ü–æ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω —É—Å–ø–µ—à–Ω–æ. –§–∞–π–ª–æ–≤ —Å–∫–∞—á–∞–Ω–æ: {len(media_paths)}")
        return result
    else:
        logger.debug("üì≠ –ü–æ—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –º–µ–¥–∏–∞-—Ñ–∞–π–ª–æ–≤")
        return None


def fetch_latest_posts(subreddit: str, limit: int = 5) -> List[Dict]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø–æ—Å—Ç–æ–≤ –∏–∑ subreddit.
    
    Args:
        subreddit: –Ω–∞–∑–≤–∞–Ω–∏–µ subreddit
        limit: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5)
        
    Returns:
        List[Dict]: —Å–ø–∏—Å–æ–∫ –ø–æ—Å—Ç–æ–≤ –≤ —Ç–æ–º –∂–µ —Ñ–æ—Ä–º–∞—Ç–µ, —á—Ç–æ –∏ fetch_latest
    """
    logger.info(f"üîç –ü–æ–ª—É—á–∞–µ–º {limit} –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø–æ—Å—Ç–æ–≤ –∏–∑ r/{subreddit}")
    api_url = f"https://www.reddit.com/r/{subreddit}.json?limit={limit}"
    headers = {"User-Agent": USER_AGENT}

    try:
        resp = requests.get(api_url, headers=headers, timeout=15)
        resp.raise_for_status()
        data = resp.json()

        children = data.get("data", {}).get("children", [])
        if not children:
            logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ—Å—Ç–æ–≤ –≤ r/{subreddit}")
            return []

        posts = []
        for child in children:
            post_data = process_reddit_post_data(child["data"])
            if post_data:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø–æ—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –º–µ–¥–∏–∞
                posts.append(post_data)
        
        logger.info(f"üìä –ü–æ–ª—É—á–µ–Ω–æ {len(posts)} –º–µ–¥–∏–∞-–ø–æ—Å—Ç–æ–≤ –∏–∑ {len(children)} –≤—Å–µ–≥–æ")
        return posts

    except requests.RequestException as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Reddit API: {e}")
        return []
    except Exception as e:
        logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –ø–æ—Å—Ç–æ–≤: {e}")
        return []


def fetch_latest(subreddit: str) -> Optional[Dict]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø–æ—Å–ª–µ–¥–Ω–µ–º –ø–æ—Å—Ç–µ.
    –¢–µ–ø–µ—Ä—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≥–∞–ª–µ—Ä–µ–∏ Reddit —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    {
        'post_id': str,
        'title': str,
        'media_type': 'image'|'video'|'gif'|'gallery'|None,
        'media_paths': List[str],  # –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º
        'is_gallery': bool
    }
    """
    posts = fetch_latest_posts(subreddit, limit=1)
    return posts[0] if posts else None