import requests
import json
import os
import logging
from urllib.parse import urlsplit, unquote
from bs4 import BeautifulSoup
from typing import List, Dict, Optional

USER_AGENT = "python:reddit.parser:v2.0 (by /u/Yar0v)"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ –¥–ª—è reddit_service
logger = logging.getLogger('reddit_service')
logger.setLevel(logging.DEBUG)


def download_media(url: str, folder: str = "downloads", index: int = 0) -> str:
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–π –º–µ–¥–∏–∞-—Ñ–∞–π–ª (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, gif, mp4) –ø–æ –ª—é–±–æ–π —Å—Å—ã–ª–∫–µ.
    –î–æ–±–∞–≤–ª–µ–Ω index –¥–ª—è —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ –≥–∞–ª–µ—Ä–µ–∏.
    """
    logger.info(f"üì• –ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–µ–¥–∏–∞ #{index} —Å URL: {url}")
    os.makedirs(folder, exist_ok=True)
    headers = {"User-Agent": USER_AGENT}

    try:
        # 1) –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å ‚Äî —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, —ç—Ç–æ —Å—Ä–∞–∑—É —Ñ–∞–π–ª –∏–ª–∏ HTML-—Å—Ç—Ä–∞–Ω–∏—Ü–∞
        logger.debug(f"üîç –í—ã–ø–æ–ª–Ω—è–µ–º GET –∑–∞–ø—Ä–æ—Å –∫ {url}")
        resp = requests.get(url, headers=headers, allow_redirects=True, timeout=15)
        resp.raise_for_status()
        ctype = resp.headers.get("Content-Type", "").lower()
        logger.debug(f"üìã Content-Type: {ctype}")

        # –ï—Å–ª–∏ —ç—Ç–æ HTML ‚Äî –∏—â–µ–º –ø—Ä—è–º–æ–π URL –Ω–∞ –º–µ–¥–∏–∞ –≤–Ω—É—Ç—Ä–∏
        if "text/html" in ctype:
            logger.debug("üåê –û–±–Ω–∞—Ä—É–∂–µ–Ω HTML, –ø–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É...")
            soup = BeautifulSoup(resp.text, "html.parser")

            # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤–∏–¥–µ–æ
            video = soup.find("video")
            if video:
                logger.debug("üé• –ù–∞–π–¥–µ–Ω —Ç–µ–≥ <video>")
                src = None
                if video.has_attr("src"):
                    src = video["src"]
                else:
                    src_tag = video.find("source")
                    if src_tag and src_tag.has_attr("src"):
                        src = src_tag["src"]
                if src:
                    logger.info(f"üîó –ù–∞–π–¥–µ–Ω –ø—Ä—è–º–æ–π URL –≤–∏–¥–µ–æ: {src}")
                    return download_media(src, folder, index)

            # Meta Open Graph: og:video –∏–ª–∏ og:image
            og = soup.find("meta", property="og:video") or soup.find("meta", property="og:image")
            if og and og.has_attr("content"):
                logger.info(f"üîó –ù–∞–π–¥–µ–Ω Open Graph URL: {og['content']}")
                return download_media(og["content"], folder, index)

            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –º–µ–¥–∏–∞ –Ω–∞ HTML-—Å—Ç—Ä–∞–Ω–∏—Ü–µ: {url}")
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –º–µ–¥–∏–∞ –Ω–∞ HTML-—Å—Ç—Ä–∞–Ω–∏—Ü–µ: {url}")

        # 2) –≠—Ç–æ —Ñ–∞–π–ª ‚Äî –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
        path = unquote(urlsplit(resp.url).path)
        base, ext = os.path.splitext(os.path.basename(path) or "file")

        # –ï—Å–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –Ω–µ—Ç, –±–µ—Ä—ë–º –∏–∑ Content-Type
        if not ext:
            if "/" in ctype:
                ext = "." + ctype.split("/")[1].split(";")[0]
            else:
                ext = ""

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å –∫ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        if index > 0:
            filename = f"{base}_{index}{ext}"
        else:
            filename = base + ext

        filepath = os.path.join(folder, filename)
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫: {filepath}")

        # 3) –°–∫–∞—á–∏–≤–∞–µ–º –ø–æ—Ç–æ–∫–æ–≤–æ
        with requests.get(resp.url, headers=headers, stream=True) as r2:
            r2.raise_for_status()
            total_size = int(r2.headers.get('content-length', 0))
            downloaded = 0

            with open(filepath, "wb") as f:
                for chunk in r2.iter_content(chunk_size=8192):
                    f.write(chunk)
                    downloaded += len(chunk)
                    if total_size > 0:
                        progress = (downloaded / total_size) * 100
                        if progress % 25 < 0.1:  # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 25%
                            logger.debug(f"‚è¨ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {progress:.1f}%")

        file_size = os.path.getsize(filepath) / 1024 / 1024  # MB
        logger.info(f"‚úÖ –§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω: {filepath} ({file_size:.2f} MB)")
        return filepath

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –º–µ–¥–∏–∞: {e}")
        raise


def fetch_latest(subreddit: str) -> Optional[Dict]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø–æ—Å–ª–µ–¥–Ω–µ–º –ø–æ—Å—Ç–µ.
    –¢–µ–ø–µ—Ä—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≥–∞–ª–µ—Ä–µ–∏ Reddit —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    {
        'post_id': str,
        'title': str,
        'media_type': 'image'|'video'|'gif'|'gallery'|None,
        'media_paths': List[str],  # –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º
        'is_gallery': bool
    }
    """
    logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ—Å—Ç –≤ r/{subreddit}")
    api_url = f"https://www.reddit.com/r/{subreddit}.json?limit=1"
    headers = {"User-Agent": USER_AGENT}

    try:
        resp = requests.get(api_url, headers=headers, timeout=15)
        resp.raise_for_status()
        data = resp.json()

        children = data.get("data", {}).get("children", [])
        if not children:
            logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ—Å—Ç–æ–≤ –≤ r/{subreddit}")
            return None

        post = children[0]["data"]
        post_id = post["name"]
        title = post.get("title", "")
        is_self = post.get("is_self", False)

        logger.info(f"üì∞ –ù–∞–π–¥–µ–Ω –ø–æ—Å—Ç: {post_id} - {title[:50]}...")
        logger.debug(f"üìä is_self: {is_self}, is_gallery: {post.get('is_gallery', False)}")

        media_paths = []
        media_type = None
        is_gallery = False

        if not is_self:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —ç—Ç–æ –≥–∞–ª–µ—Ä–µ—è?
            if post.get("is_gallery", False):
                logger.info("üñºÔ∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –≥–∞–ª–µ—Ä–µ—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
                is_gallery = True
                media_type = "gallery"

                # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≥–∞–ª–µ—Ä–µ–∏
                gallery_data = post.get("gallery_data", {})
                media_metadata = post.get("media_metadata", {})

                if gallery_data and media_metadata:
                    items = gallery_data.get("items", [])
                    logger.info(f"üì∏ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –≥–∞–ª–µ—Ä–µ–µ: {len(items)}")

                    for idx, item in enumerate(items):
                        media_id = item.get("media_id")
                        if media_id and media_id in media_metadata:
                            media_info = media_metadata[media_id]

                            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                            if "s" in media_info:
                                # –ü–æ–ª—É—á–∞–µ–º URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                if "u" in media_info["s"]:
                                    # URL –≤ —Ñ–æ—Ä–º–∞—Ç–µ preview, –Ω—É–∂–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å
                                    preview_url = media_info["s"]["u"]
                                    # –ó–∞–º–µ–Ω—è–µ–º preview.redd.it –Ω–∞ i.redd.it –∏ —É–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                                    image_url = preview_url.replace("preview.redd.it", "i.redd.it")
                                    image_url = image_url.split("?")[0].replace("&amp;", "&")
                                elif "gif" in media_info["s"]:
                                    image_url = media_info["s"]["gif"]
                                else:
                                    continue

                                logger.debug(f"üîó URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è #{idx}: {image_url}")

                                try:
                                    media_path = download_media(image_url, index=idx)
                                    media_paths.append(media_path)
                                except Exception as e:
                                    logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ #{idx}: {e}")
                else:
                    logger.error("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –¥–∞–Ω–Ω—ã–µ –≥–∞–ª–µ—Ä–µ–∏")
            else:
                # –û–±—ã—á–Ω—ã–π –ø–æ—Å—Ç —Å –æ–¥–Ω–∏–º –º–µ–¥–∏–∞
                url = post.get("url")
                logger.info(f"üìé –û–±—ã—á–Ω—ã–π –º–µ–¥–∏–∞-–ø–æ—Å—Ç: {url}")

                try:
                    media_path = download_media(url)
                    media_paths = [media_path]

                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–µ–¥–∏–∞
                    ext = media_path.lower().split('.')[-1]
                    if ext in ("jpg", "jpeg", "png"):
                        media_type = "image"
                    elif ext == "mp4":
                        media_type = "video"
                    elif ext == "gif":
                        media_type = "gif"

                    logger.info(f"üìã –¢–∏–ø –º–µ–¥–∏–∞: {media_type}")
                except Exception as e:
                    logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –º–µ–¥–∏–∞: {e}")
                    return None

        result = {
            "post_id": post_id,
            "title": title,
            "media_type": media_type,
            "media_paths": media_paths,
            "is_gallery": is_gallery
        }

        logger.info(f"‚úÖ –ü–æ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω —É—Å–ø–µ—à–Ω–æ. –§–∞–π–ª–æ–≤ —Å–∫–∞—á–∞–Ω–æ: {len(media_paths)}")
        return result

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø–æ—Å—Ç–∞ –∏–∑ r/{subreddit}: {e}")
        return None